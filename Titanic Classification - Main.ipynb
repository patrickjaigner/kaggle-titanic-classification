{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":200,"outputs":[{"output_type":"stream","text":"/kaggle/input/titanic/gender_submission.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference to public kaggle kernels licensed under Apache 2.0 open source license\n# Some cells are inspired by the tutorials of Dante, Yassine Ghouzam and LD Freeman","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data handling\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# ML algorithms\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost.sklearn import XGBClassifier\n\n# Model selection and analysis tools\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.feature_selection import SelectFromModel\nfrom scipy.stats import randint\nimport itertools\nfrom sklearn.metrics import confusion_matrix, roc_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score","execution_count":201,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_raw = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_raw = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":202,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy Data to clean it\ntrain_data = train_raw.copy()\ntest_data = test_raw.copy()\n\n# Create List for easy access on both datasets\nclean_train_test = [train_data, test_data]","execution_count":203,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Analysis\nLet's have a first look into the DataFrame. \n\n**Competition Description**\n\n> The sinking of the Titanic is one of the most infamous shipwrecks in history.\n>\n>On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n>\n>The data has been split into two groups:\n>\n>training set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n>\n>The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n>\n>We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n>\n>age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n>\n>sibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\n>\n>parch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_raw.info())\nprint(train_raw.describe())","execution_count":204,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nNone\n       PassengerId    Survived      Pclass         Age       SibSp  \\\ncount   891.000000  891.000000  891.000000  714.000000  891.000000   \nmean    446.000000    0.383838    2.308642   29.699118    0.523008   \nstd     257.353842    0.486592    0.836071   14.526497    1.102743   \nmin       1.000000    0.000000    1.000000    0.420000    0.000000   \n25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n75%     668.500000    1.000000    3.000000   38.000000    1.000000   \nmax     891.000000    1.000000    3.000000   80.000000    8.000000   \n\n            Parch        Fare  \ncount  891.000000  891.000000  \nmean     0.381594   32.204208  \nstd      0.806057   49.693429  \nmin      0.000000    0.000000  \n25%      0.000000    7.910400  \n50%      0.000000   14.454200  \n75%      0.000000   31.000000  \nmax      6.000000  512.329200  \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Investigate missing data cells**\n\nLet's search for every cell that has NaN. This will show us how complete the dataset is and what data cells we might need to fill."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze for missing data\n\nfor dataset in clean_train_test:\n    print(\"\")\n    for key in list(dataset.columns.values):\n        gap = pd.isnull(dataset[key]).sum()\n        gap_percentage = gap / len(dataset) * 100\n        print(\"{:.1f}% of {} data is missing\".format(gap_percentage, key))","execution_count":205,"outputs":[{"output_type":"stream","text":"\n0.0% of PassengerId data is missing\n0.0% of Survived data is missing\n0.0% of Pclass data is missing\n0.0% of Name data is missing\n0.0% of Sex data is missing\n19.9% of Age data is missing\n0.0% of SibSp data is missing\n0.0% of Parch data is missing\n0.0% of Ticket data is missing\n0.0% of Fare data is missing\n77.1% of Cabin data is missing\n0.2% of Embarked data is missing\n\n0.0% of PassengerId data is missing\n0.0% of Pclass data is missing\n0.0% of Name data is missing\n0.0% of Sex data is missing\n20.6% of Age data is missing\n0.0% of SibSp data is missing\n0.0% of Parch data is missing\n0.0% of Ticket data is missing\n0.2% of Fare data is missing\n78.2% of Cabin data is missing\n0.0% of Embarked data is missing\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"For age and cabin data a significant share is missing. We'll see if it is possible to fill these.\nFor Embarked data and Fare data only few cells are missing. We probably can fill them right away."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include = 'all')","execution_count":206,"outputs":[{"output_type":"execute_result","execution_count":206,"data":{"text/plain":"        PassengerId    Survived      Pclass  \\\ncount    891.000000  891.000000  891.000000   \nunique          NaN         NaN         NaN   \ntop             NaN         NaN         NaN   \nfreq            NaN         NaN         NaN   \nmean     446.000000    0.383838    2.308642   \nstd      257.353842    0.486592    0.836071   \nmin        1.000000    0.000000    1.000000   \n25%      223.500000    0.000000    2.000000   \n50%      446.000000    0.000000    3.000000   \n75%      668.500000    1.000000    3.000000   \nmax      891.000000    1.000000    3.000000   \n\n                                                     Name   Sex         Age  \\\ncount                                                 891   891  714.000000   \nunique                                                891     2         NaN   \ntop     Laroche, Mrs. Joseph (Juliette Marie Louise La...  male         NaN   \nfreq                                                    1   577         NaN   \nmean                                                  NaN   NaN   29.699118   \nstd                                                   NaN   NaN   14.526497   \nmin                                                   NaN   NaN    0.420000   \n25%                                                   NaN   NaN   20.125000   \n50%                                                   NaN   NaN   28.000000   \n75%                                                   NaN   NaN   38.000000   \nmax                                                   NaN   NaN   80.000000   \n\n             SibSp       Parch Ticket        Fare Cabin Embarked  \ncount   891.000000  891.000000    891  891.000000   204      889  \nunique         NaN         NaN    681         NaN   147        3  \ntop            NaN         NaN   1601         NaN    G6        S  \nfreq           NaN         NaN      7         NaN     4      644  \nmean      0.523008    0.381594    NaN   32.204208   NaN      NaN  \nstd       1.102743    0.806057    NaN   49.693429   NaN      NaN  \nmin       0.000000    0.000000    NaN    0.000000   NaN      NaN  \n25%       0.000000    0.000000    NaN    7.910400   NaN      NaN  \n50%       0.000000    0.000000    NaN   14.454200   NaN      NaN  \n75%       1.000000    0.000000    NaN   31.000000   NaN      NaN  \nmax       8.000000    6.000000    NaN  512.329200   NaN      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891</td>\n      <td>891</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891</td>\n      <td>891.000000</td>\n      <td>204</td>\n      <td>889</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>891</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>681</td>\n      <td>NaN</td>\n      <td>147</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Laroche, Mrs. Joseph (Juliette Marie Louise La...</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1601</td>\n      <td>NaN</td>\n      <td>G6</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>577</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>644</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>NaN</td>\n      <td>32.204208</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>NaN</td>\n      <td>49.693429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>7.910400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>14.454200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>31.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>NaN</td>\n      <td>512.329200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 2. Clean Data and Fill Missing Data Gaps\n\nLet's see what info we can extract from the already available features. As there is a lot of social data we will try to get more detailed info on every person and detect their relations to other passengers. \n\nFor the Embarked data we will detect the most commong value and replace the empty cell with it. For the missing Fare cell we will calculate the median value over all cells. \n\nAdditionally we will calculate the family size by adding Parch and SibSp + 1.\n\nFor the Cabin Info we will drop the Number and just save the deck (first letter of the cabin) to a new column.\n\nLastly we detect every passenger that is traveling alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing data\n\ndef cabin_into_deck(x):\n    if x['Cabin'][0] == 'T':\n        return 'A' #T is even higher than A\n    return x['Cabin'][0]\n\nfor dataset in clean_train_test:\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].mean(), inplace = True)\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['Cabin'].fillna('X', inplace = True)\n    dataset['Deck'] = dataset.apply(cabin_into_deck, axis = 1)\n\n\nfor dataset in clean_train_test:\n    dataset.loc[:,'TravAlone'] = 0\n    dataset['TravAlone'].loc[dataset['FamilySize'] == 1] = 1","execution_count":207,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature from family size\nfor dataset in clean_train_test:\n    dataset['SmallF'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n    dataset['MedF'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n    dataset['LargeF'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","execution_count":208,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group the decks together. A is the highest deck while \nkey = 'Deck'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\", order=['A','B','C','D','E','F','G','X'])\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":209,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    dataset['Deck_A'] = dataset['Deck'].loc[(dataset['Deck'] == 'A')]\n    dataset['Deck_BCDE'] = dataset['Deck'].loc[(dataset['Deck'] == 'B') | (dataset['Deck'] == 'C')\n                                               | (dataset['Deck'] == 'D') | (dataset['Deck'] == 'E')]\n    dataset['Deck_FG'] = dataset['Deck'].loc[(dataset['Deck'] == 'F') | (dataset['Deck'] == 'G')]\n    dataset['Deck_X'] = dataset['Deck'].loc[(dataset['Deck'] == 'X')]","execution_count":210,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fare data**\n\nWhen looking deeper into the fare data we will realize that families/groups sharing a cabin have the same fare prize in their data cell. Passengers from the third class have similar values to person in the first class. To get more realistic fare prise per person we will detect groups of people sharing a cabin by grouping the dataset by the ticket number. \n\nAfterwards we will devide the fare price by the group size. This will be safed in a new column \"FareCorr\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    # look for person with the same ticket and divide the fare price by the number of persons\n    dataset['FareCorr'] = dataset['Fare'].copy()\n    \n    for _, grp_df in dataset[['Ticket', 'Fare', 'PassengerId']].groupby(['Ticket']):\n\n        if (len(grp_df) != 1):\n            for _, row in grp_df.iterrows():\n                passID = row['PassengerId']\n                dataset['FareCorr'].loc[dataset['PassengerId'] == passID] = dataset['Fare'][dataset['PassengerId'] \n                                                                                            == passID] / len(grp_df)","execution_count":211,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make the data more accessable by our machine learning algoritmn we will thin the dataset to different bins and save it into the new column \"FarCat\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    dataset['FareCat']=pd.cut(dataset['FareCorr'], bins=[0,15,65,max(dataset[\"FareCorr\"]+1)], labels=['low','mid','high'])","execution_count":212,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if \"FareCat\" is complete\ntrain_data.loc[train_data['FareCat'].isna() == True]","execution_count":213,"outputs":[{"output_type":"execute_result","execution_count":213,"data":{"text/plain":"     PassengerId  Survived  Pclass                              Name   Sex  \\\n179          180         0       3               Leonard, Mr. Lionel  male   \n263          264         0       1             Harrison, Mr. William  male   \n271          272         1       3      Tornquist, Mr. William Henry  male   \n277          278         0       2       Parkes, Mr. Francis \"Frank\"  male   \n302          303         0       3   Johnson, Mr. William Cahoone Jr  male   \n413          414         0       2    Cunningham, Mr. Alfred Fleming  male   \n466          467         0       2             Campbell, Mr. William  male   \n481          482         0       2  Frost, Mr. Anthony Wood \"Archie\"  male   \n597          598         0       3               Johnson, Mr. Alfred  male   \n633          634         0       1     Parr, Mr. William Henry Marsh  male   \n674          675         0       2        Watson, Mr. Ennis Hastings  male   \n732          733         0       2              Knight, Mr. Robert J  male   \n806          807         0       1            Andrews, Mr. Thomas Jr  male   \n815          816         0       1                  Fry, Mr. Richard  male   \n822          823         0       1   Reuchlin, Jonkheer. John George  male   \n\n      Age  SibSp  Parch  Ticket  Fare  ... TravAlone SmallF  MedF LargeF  \\\n179  36.0      0      0    LINE   0.0  ...         1      0     0      0   \n263  40.0      0      0  112059   0.0  ...         1      0     0      0   \n271  25.0      0      0    LINE   0.0  ...         1      0     0      0   \n277   NaN      0      0  239853   0.0  ...         1      0     0      0   \n302  19.0      0      0    LINE   0.0  ...         1      0     0      0   \n413   NaN      0      0  239853   0.0  ...         1      0     0      0   \n466   NaN      0      0  239853   0.0  ...         1      0     0      0   \n481   NaN      0      0  239854   0.0  ...         1      0     0      0   \n597  49.0      0      0    LINE   0.0  ...         1      0     0      0   \n633   NaN      0      0  112052   0.0  ...         1      0     0      0   \n674   NaN      0      0  239856   0.0  ...         1      0     0      0   \n732   NaN      0      0  239855   0.0  ...         1      0     0      0   \n806  39.0      0      0  112050   0.0  ...         1      0     0      0   \n815   NaN      0      0  112058   0.0  ...         1      0     0      0   \n822  38.0      0      0   19972   0.0  ...         1      0     0      0   \n\n     Deck_A  Deck_BCDE  Deck_FG  Deck_X FareCorr FareCat  \n179     NaN        NaN      NaN       X      0.0     NaN  \n263     NaN          B      NaN     NaN      0.0     NaN  \n271     NaN        NaN      NaN       X      0.0     NaN  \n277     NaN        NaN      NaN       X      0.0     NaN  \n302     NaN        NaN      NaN       X      0.0     NaN  \n413     NaN        NaN      NaN       X      0.0     NaN  \n466     NaN        NaN      NaN       X      0.0     NaN  \n481     NaN        NaN      NaN       X      0.0     NaN  \n597     NaN        NaN      NaN       X      0.0     NaN  \n633     NaN        NaN      NaN       X      0.0     NaN  \n674     NaN        NaN      NaN       X      0.0     NaN  \n732     NaN        NaN      NaN       X      0.0     NaN  \n806       A        NaN      NaN     NaN      0.0     NaN  \n815     NaN          B      NaN     NaN      0.0     NaN  \n822     NaN        NaN      NaN       X      0.0     NaN  \n\n[15 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>...</th>\n      <th>TravAlone</th>\n      <th>SmallF</th>\n      <th>MedF</th>\n      <th>LargeF</th>\n      <th>Deck_A</th>\n      <th>Deck_BCDE</th>\n      <th>Deck_FG</th>\n      <th>Deck_X</th>\n      <th>FareCorr</th>\n      <th>FareCat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>179</th>\n      <td>180</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Leonard, Mr. Lionel</td>\n      <td>male</td>\n      <td>36.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>LINE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>263</th>\n      <td>264</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Harrison, Mr. William</td>\n      <td>male</td>\n      <td>40.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112059</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>B</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>272</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Tornquist, Mr. William Henry</td>\n      <td>male</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>LINE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>278</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Parkes, Mr. Francis \"Frank\"</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239853</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>303</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Johnson, Mr. William Cahoone Jr</td>\n      <td>male</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>LINE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>414</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Cunningham, Mr. Alfred Fleming</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239853</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>466</th>\n      <td>467</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Campbell, Mr. William</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239853</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>481</th>\n      <td>482</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Frost, Mr. Anthony Wood \"Archie\"</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239854</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>597</th>\n      <td>598</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Johnson, Mr. Alfred</td>\n      <td>male</td>\n      <td>49.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>LINE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>633</th>\n      <td>634</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Parr, Mr. William Henry Marsh</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112052</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>674</th>\n      <td>675</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Watson, Mr. Ennis Hastings</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239856</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>732</th>\n      <td>733</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Knight, Mr. Robert J</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239855</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>807</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Andrews, Mr. Thomas Jr</td>\n      <td>male</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112050</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>A</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>815</th>\n      <td>816</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Fry, Mr. Richard</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112058</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>B</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>822</th>\n      <td>823</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Reuchlin, Jonkheer. John George</td>\n      <td>male</td>\n      <td>38.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19972</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>X</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Apparently there are some cells with fare data == 0 that were not cut with the bins. Let's put them into \"mid\" and \"low\" categories depening on the passenger class. These are either crew members or did somehow get the trip for free. Let's not assume that these compare to high class fare prizes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    dataset['FareCat'].loc[(dataset['FareCat'].isna() == True) & (dataset['Pclass'] == 1)] = 'low'\n    dataset['FareCat'].loc[(dataset['FareCat'].isna() == True) & (dataset['Pclass'] > 1)] = 'mid'","execution_count":214,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Titles**\n\nThe names category give a lot of room for feature engineering. The most obvious is probably extracting the titles. From these titles we will be able to differenciate from younger and older people and maybe from people of higher social class. We will also use these titles later to calculate the median age for different passenger groups. The goals is not to create too many different titles to allow for a better learing as our dataset is limited in size."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract titles from names to get more possible features to learn age\n\ndef replace_name_with_title(x):\n    \"\"\"Searches for titles in names and extracts them in a new column Title\"\"\"\n    \n    line = x['Name']\n    \n    #title_mr = ['Mr', 'Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']\n    title_mr = ['Mr']\n    title_mrs = ['Mrs', 'Mme']\n    title_miss = ['Miss', 'Mlle', 'Ms']\n    title_others = ['Sir','Don','Dona','Jonkheer','Lady','Countess','Dr', 'Rev','Col','Major','Capt']\n    \n    for title in title_others:\n        if title in line:\n            return 'Other'\n    \n    for title in title_mrs:\n        if title in line:\n            return 'Mrs'\n        \n    for title in title_mr:\n        if title in line:\n            return 'Mr'\n        \n    if 'Master' in line:\n        return 'Master'\n        \n    for title in title_miss:\n        if title in line:\n            return 'Miss'\n        \n    return np.nan\n\nfor dataset in clean_train_test:\n    \n    #df = dataset.copy()      \n    dataset['Title'] = dataset.apply(replace_name_with_title, axis=1) \n    \nprint(train_data['Title'].value_counts())\nprint(train_data['Title'].isna().sum())","execution_count":215,"outputs":[{"output_type":"stream","text":"Mr        506\nMiss      180\nMrs       127\nMaster     40\nOther      38\nName: Title, dtype: int64\n0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Let's see how these median age claculations compare. Also as there is no equivalent for \"Master\" we will split the title \"Miss\" into \"Young Miss\" (travels with parents) and \"Miss\" (travels without parents and children)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mean_age():\n    # Calculate mean age values for different titles\n    print('Mr')\n    print(train_data['Age'].loc[train_data['Title'] == 'Mr'].median())\n    print('Master')\n    print(train_data['Age'].loc[train_data['Title'] == 'Master'].median())\n    print('Mrs')\n    print(train_data['Age'].loc[train_data['Title'] == 'Mrs'].median())\n    print('Young Miss')\n    print(train_data['Age'].loc[(train_data['Title'] == 'Miss') & (train_data['Parch'] != 0) & (train_data['Age'] < 15)].median())\n    print('Miss')\n    print(train_data['Age'].loc[(train_data['Title'] == 'Miss') & (train_data['Parch'] == 0)].median())\n    print('Other')\n    print(train_data['Age'].loc[train_data['Title'] == 'Other'].median())\n    \nprint_mean_age()","execution_count":216,"outputs":[{"output_type":"stream","text":"Mr\n29.5\nMaster\n3.5\nMrs\n35.0\nYoung Miss\n4.0\nMiss\n26.0\nOther\n40.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that \"Young Miss\" and \"Miss\" have very different age medians. Therefore we will proceed and create a new title called \"Young Miss\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n\n    # Fill missing age value with mean values according to title\n    mean_age_mr = dataset['Age'].loc[dataset['Title'] == 'Mr'].median()\n    mean_age_master = dataset['Age'].loc[dataset['Title'] == 'Master'].median()\n    mean_age_mrs = dataset['Age'].loc[dataset['Title'] == 'Mrs'].median()\n    mean_age_miss = dataset['Age'].loc[dataset['Title'] == 'Miss'].median()\n    mean_age_youngmiss = dataset['Age'].loc[(dataset['Title'] == 'Miss') & (dataset['Parch'] != 0)\n                                           & (dataset['Age'] < 15)].median()\n    mean_age_other = dataset['Age'].loc[dataset['Title'] == 'Other'].median()\n    \n    dataset['Title'].loc[(dataset['Title'] == 'Miss') & (dataset['Parch'] != 0)] = 'YoungMiss'\n    \n    print(mean_age_mr,mean_age_master,mean_age_mrs,mean_age_miss, mean_age_youngmiss, mean_age_other)\n    \n    dataset['Age'].loc[(dataset['Title'] == 'Mr') & (dataset['Age'].isna() == True)]  = mean_age_mr\n    dataset['Age'].loc[(dataset['Title'] == 'Master') & (dataset['Age'].isna() == True)] = mean_age_master\n    dataset['Age'].loc[(dataset['Title'] == 'Mrs') & (dataset['Age'].isna() == True)] =  mean_age_mrs\n    dataset['Age'].loc[(dataset['Title'] == 'Miss') & (dataset['Age'].isna() == True)]  = mean_age_miss\n    dataset['Age'].loc[(dataset['Title'] == 'YoungMiss') & (dataset['Age'].isna() == True)] = mean_age_youngmiss\n    dataset['Age'].loc[(dataset['Title'] == 'Other') & (dataset['Age'].isna() == True)] =  mean_age_other\n","execution_count":217,"outputs":[{"output_type":"stream","text":"29.5 3.5 35.0 21.0 4.0 40.0\n29.0 6.5 36.0 22.0 2.0 39.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We all know the rule \"Women and Children First\". It's probably reasonable that women and children have a higher chance of survival. We will extract an additional label \"Child\" from all the female and male passengers. I guess that it doesn't really matter if the child is male or female. We will later check if that feature give more insight than sex alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    \n    dataset['MFK'] = dataset['Sex']\n    dataset['MFK'].loc[dataset['Age'] <15] = 'Child'","execution_count":218,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also thin out the \"Age\" feature 5 different age groups. This will probably speed up our Machine Learning Algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    dataset[\"AgeCat\"]= pd.cut(dataset[\"Age\"], bins=[0,14.9,30,45,60,max(dataset[\"Age\"]+1)], labels=['1','2','3','4','5'])","execution_count":219,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the feature \"Pclass\" is not continous we will save it into three different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    \n    dataset['Pclass_1'] = dataset['Pclass'].loc[dataset['Pclass'] == 1]\n    dataset['Pclass_1'].fillna(0, inplace = True)\n    dataset['Pclass_2'] = dataset['Pclass'].loc[dataset['Pclass'] == 2] / 2\n    dataset['Pclass_2'].fillna(0, inplace = True)\n    dataset['Pclass_3'] = dataset['Pclass'].loc[dataset['Pclass'] == 3] / 3\n    dataset['Pclass_3'].fillna(0, inplace = True)\n","execution_count":220,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Family Survival**\n\nThis is a long shot and might be seen as a little hack. It is only possible because our training and test set will not be growing over time. The thinking behind this is that families probably stay together and if they made it to a rescue ship they probably all lifed. We will detect families in both datasets and group them together. So let's add them together with concat.\n\nAfterwards we will check if a majority of the family survied and put the \"Family Survival\" to 1. If the majority dies we will put 0. If we are unsure (the majority is in the test set where the label survival is not set) we will put it to 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_data, test_data], axis=0, ignore_index=True, sort=True)\n    \n# Extract the last name. \ndata['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Set a default value for all passengers\ndata['FamilySurvival'] = 0.5\n\n# Group by Last Name\nfor _, grp_df in data[['Survived', 'Last_Name', 'Ticket', 'PassengerId']].groupby(['Last_Name']):\n\n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data['FamilySurvival'].loc[data['PassengerId'] == passID] = 1\n            elif (smin == 0.0):\n                data['FamilySurvival'].loc[data['PassengerId'] == passID] = 0\n\n#Group by ticket\nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        #print(grp_df)\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival'] == 0) | (row['FamilySurvival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data['FamilySurvival'].loc[data['PassengerId'] == passID] = 1\n                elif (smin == 0.0):\n                    data['FamilySurvival'].loc[data['PassengerId'] == passID] = 0\n\n# Check number of passengers in the different categories\ndata['FamilySurvival'].value_counts() ","execution_count":222,"outputs":[{"output_type":"execute_result","execution_count":222,"data":{"text/plain":"0.5    656\n1.0    376\n0.0    277\nName: FamilySurvival, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now we need to get this info back into our train and test dataframes. Let's map them by PassengerID."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    dataset['FamilySurvival'] = 0.5\n\nfor dataset in clean_train_test:\n    for passid in range(int(dataset['PassengerId'][0]), int(dataset['PassengerId'][-1:])+1):\n        dataset['FamilySurvival'].loc[dataset['PassengerId'] == passid] = float(data[\n            'FamilySurvival'].loc[data['PassengerId'] == passid])\n\n    print(dataset['FamilySurvival'].value_counts())","execution_count":223,"outputs":[{"output_type":"stream","text":"0.5    453\n1.0    251\n0.0    187\nName: FamilySurvival, dtype: int64\n0.5    203\n1.0    125\n0.0     90\nName: FamilySurvival, dtype: int64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Again we will create three different features for 1, 0.5 and 0 to help our ML algortihm."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n    \n    dataset['FamilySurvival_1'] = dataset['FamilySurvival'].loc[dataset['FamilySurvival'] == 1.0]\n    dataset['FamilySurvival_1'].fillna(0,inplace = True)\n    dataset['FamilySurvival_0.5'] = dataset['FamilySurvival'].loc[dataset['FamilySurvival'] == 0.5] +0.5\n    dataset['FamilySurvival_0.5'].fillna(0,inplace = True)\n    dataset['FamilySurvival_0'] = dataset['FamilySurvival'].loc[dataset['FamilySurvival'] == 0] +1\n    dataset['FamilySurvival_0'].fillna(0,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze for missing data\n\nfor dataset in clean_train_test:\n    print(\"\")\n    for key in list(dataset.columns.values):\n        gap = pd.isnull(dataset[key]).sum()\n        gap_percentage = gap / len(dataset) * 100\n        print(\"{:.1f}% of {} data is missing\".format(gap_percentage, key))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Analysis\n\nIn this section we will dive deeper into each existing and engineered feature to decide which to keep and which to drop."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Subset of all available features\nfeatures = list(train_data.columns.values)\nfeatures.remove('Survived')\n\nfor key in features:\n\n    if train_data[key].dtype != 'float64' :\n        print('Survival rate for feature \"{}\"'.format(key))\n        print(train_data.groupby(key, as_index=True).mean()['Survived'])\n        print('----')\n        print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'Pclass'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'FamilySurvival'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This works better than expected. It seems that family survival is indeed a thing."},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'Title'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Women and children first\" and maybe some nobles"},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'FamilySize'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=train_data,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=train_data,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=train_data,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows significant differences. Let prioritize it over \"Family Size\""},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'TravAlone'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'Embarked'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'Sex'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'MFK'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I like how the children give more value. Also cuts alot of young males out of the picture. Let's drop \"Sex\" for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'SibSp'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'Parch'\n\n# Explore SibSp feature vs Survived\ng = sns.factorplot(x=key,y=\"Survived\",data=train_data,kind=\"bar\", size = 6 , \n                   palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Age vs Survived\ng = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Age distibution \ng = sns.kdeplot(train_data[\"Age\"][(train_data[\"Survived\"] == 0)], color=\"Red\", shade = True)\ng = sns.kdeplot(train_data[\"Age\"][(train_data[\"Survived\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Age vs Survived\ng = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in clean_train_test:\n\n    # Apply log to Fare to reduce skewness distribution\n    dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Age distibution \ng = sns.kdeplot(train_data[\"Fare\"][(train_data[\"Survived\"] == 0)], color=\"Red\", shade = True)\ng = sns.kdeplot(train_data[\"Fare\"][(train_data[\"Survived\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Fare\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features selection**\n\nLet's see what features have built up and think about what we want to keep and what to drop."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_data.columns.values)\nfeatures.remove('Survived')\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\ng = sns.heatmap(dataset[features].corr(),cmap=\"BrBG\",annot=True, square=True, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop features that didn't make the cut.\nfeatures.remove('PassengerId')\nfeatures.remove('Name')\nfeatures.remove('Parch')\nfeatures.remove('SibSp')\nfeatures.remove('FamilySize')\n#features.remove('SmallF')\n#features.remove('MedF')\n#features.remove('LargeF')\nfeatures.remove('Cabin')\nfeatures.remove('Ticket')\nfeatures.remove('FareCorr')\nfeatures.remove('Fare')\nfeatures.remove('Deck')\nfeatures.remove('Age')\nfeatures.remove('Sex')\nfeatures.remove('Pclass') \nfeatures.remove('FamilySurvival') \n#features.remove('MFK') \nfeatures.remove('Deck_X')\nfeatures.remove('Deck_A')\n\n\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our final list of features. I tuned this list quite alot and tries alot of different versions. Finally I stayed with this set."},{"metadata":{},"cell_type":"markdown","source":"# 4. Model Analysis and Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalization of big values to range of 10\ndef normalization(col, n=1):\n    \"\"\"Normalized whole column. Takes col pandas series. n is the range\"\"\"\n    \n    col=((col-col.min())/(col.max()-col.min()))*n\n    return col\n\nfor dataset in clean_train_test:\n    dataset['Age'] = normalization(dataset['Age'],1)\n    dataset['Fare'] = normalization(dataset['Fare'],1)\n    dataset['FareCorr'] = normalization(dataset['FareCorr'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\ny = train_data[\"Survived\"]\n\nprint('X and X_Test shape')\nprint(X.shape)\nprint(X_test.shape)\n\nif list(X.columns.values) == list(X_test.columns.values):\n    print('Features in both datasets match.')\nelse:\n    print('Features in both datasets do not match.')\n\n#split the train data\ntrain1_x, test1_x, train1_y, test1_y = train_test_split(X, y, random_state = 0)\n\nprint('Test and Validation shape from train_test_split of X')\nprint(train1_x.shape)\nprint(train1_y.shape)\nprint(test1_x.shape)\nprint(test1_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA to reduce dimenson to 2 for plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\n\npca = PCA(n_components=2)\npca.fit(X)\nprint(pca.explained_variance_)\n#print(pca.components_)\n\nX_trans = pca.fit_transform(X)\nprint(X.shape)\nprint(X_trans.shape)\ntype(X_trans)\n\nplt.scatter(X_trans[:, 0], X_trans[:, 1],c=y, cmap='rainbow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.2 Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X, y = y, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\n                                                                                      \"AdaBoost\",\n                                                                                      \"RandomForest\",\n                                                                                      \"ExtraTrees\",\n                                                                                      \"GradientBoosting\",\n                                                                                      \"MultipleLayerPerceptron\",\n                                                                                      \"KNeighboors\",\n                                                                                      \"LogisticRegression\",\n                                                                                      \"LinearDiscriminantAnalysis\",\n                                                                                      \"XGBClassifier\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's choose some ML algortims from this list to do some hyperparameter tuning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    \n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'],  #['rbf', 'linear','poly', 'sigmoid']\n                  'gamma': [0.01, 0.02, 0.03, 0.05], #[ 0.001, 0.01, 0.1, 1]\n                  'C': [1, 10, 50, 100, 200]} #[1, 10, 50, 100,200,300, 1000]\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, n_jobs= 4, verbose = 1, return_train_score=True, refit=True)\n\ngsSVMC.fit(X,y)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\nprint(gsSVMC.best_score_)\nprint(gsSVMC.best_params_)\n\n\n#{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X,y,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [50, 100, 200, 300],\n              'learning_rate': [0.001, 0.01, 0.05, 1],\n              'max_depth': [4, 5],\n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X,y)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\nprint(gsGBC.best_score_)\nprint(gsGBC.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X,y,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [5,6,7],\n              \"max_features\": ['auto'],\n              \n               }\n\n            #\"min_samples_split\": [3,10,15],\n           #   \"min_samples_leaf\": [18, 20, 22],\n            #  \"bootstrap\": [False],\n            #  \"n_estimators\" :[80, 100, 120],\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X,y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\nprint(gsRFC.best_score_)\nprint(gsRFC.best_params_)\n\n#best_params_{'bootstrap': True, 'max_depth': 5, 'max_features': 'auto', \n#'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X,y,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None, 3, 4, 5],\n              \"max_features\": ['auto', 10, 30],}\n              #\"min_samples_split\": [2, 4],\n              #\"min_samples_leaf\": [1, 5],\n              #\"bootstrap\": [False],\n              #\"n_estimators\" :[200],\n              #\"criterion\": [\"gini\", \"entropy\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X,y)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\nprint(gsExtC.best_score_)\nprint(gsExtC.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(gsExtC.best_estimator_,\"Extra Tree mearning curves\",X,y,cv=kfold)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearDiscriminantAnalysis\nLDA = LinearDiscriminantAnalysis()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"solver\": ['svd,''lsqr', 'eigen'],\n              \"shrinkage\": ['auto', None],\n              \"n_components\": [None, 1, 10, 100],\n              }\n\n\ngsLDA = GridSearchCV(LDA,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsLDA.fit(X,y)\n\nLDA_best = gsLDA.best_estimator_\n\n# Best score\nprint(gsLDA.best_score_)\nprint(gsLDA.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(gsLDA.best_estimator_,\"Linear Disciminant Analysis mearning curves\",X,y,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Esembly"},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('gbc',GBC_best), ('extc', ExtC_best),\n('svc', SVMC_best),  ('rfc', RFC_best), ('LDA', LDA_best)], voting='soft', n_jobs=4)  # ('gbc',GBC_best),\n\n#SVMC_best\n#GBC_best\n#RFC_best\n#LREG_best\n#ExtC_best\n\nvotingC = votingC.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the end Gradient Boost Clasifier did outperform all others and even did better than the esembly. So let's keep it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = SVC(probability= True, C= 100, gamma= 0.01, kernel= 'rbf')\n#model = RandomForestClassifier(bootstrap=False, max_depth=5, max_features='auto',\n#                               min_samples_leaf=4,min_samples_split=10,n_estimators=150)\n\n#model.fit(X,y)\n\npredictions = gsGBC.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_version34.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}